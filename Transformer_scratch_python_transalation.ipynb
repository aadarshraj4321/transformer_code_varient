{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "V28"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tIPgeJkLMyUb"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import math\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader, random_split\n",
        "from torch.utils.tensorboard import SummaryWriter"
      ],
      "metadata": {
        "id": "0tnuN8E5NZaa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "from tokenizers import Tokenizer\n",
        "from tokenizers.models import WordLevel\n",
        "from tokenizers.trainers import WordLevelTrainer\n",
        "from tokenizers.pre_tokenizers import Whitespace"
      ],
      "metadata": {
        "id": "hKPMBvLBPVXX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pathlib import Path\n",
        "from typing import Any\n",
        "from tqdm import tqdm\n",
        "import warnings"
      ],
      "metadata": {
        "id": "nksY8LHcPVaJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "0s4BDnK5PVce"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class InputEmbeddings(nn.Module):\n",
        "    def __init__(self, d_model: int, vocab_size: int):\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "        self.vocab_size = vocab_size\n",
        "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.embedding(x) * math.sqrt(self.d_model)\n"
      ],
      "metadata": {
        "id": "UxNJXl1lPVe2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, d_model: int, seq_len: int, dropout: float) -> None:\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "        self.seq_len = seq_len\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "        pe = torch.zeros(seq_len, d_model)\n",
        "\n",
        "        position = torch.arange(0, seq_len, dtype=torch.float).unsqueeze(1)\n",
        "\n",
        "\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
        "\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "\n",
        "        pe = pe.unsqueeze(0)\n",
        "\n",
        "        self.register_buffer('pe', pe)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + (self.pe[:, :x.shape[1], :]).requires_grad_(False)\n",
        "        return self.dropout(x)\n"
      ],
      "metadata": {
        "id": "3eLn8EKyPVhG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class LayerNormalization(nn.Module):\n",
        "    def __init__(self, eps: float = 10**-6) -> None:\n",
        "        super().__init__()\n",
        "        self.eps = eps\n",
        "\n",
        "        self.alpha = nn.Parameter(torch.ones(1))\n",
        "\n",
        "        self.bias = nn.Parameter(torch.zeros(1))\n",
        "\n",
        "    def forward(self, x):\n",
        "        if(x is None):\n",
        "            raise ValueError(\"Input x is None\")\n",
        "        mean = x.mean(dim = -1, keepdim=True)\n",
        "        std = x.std(dim = -1, keepdim=True)\n",
        "\n",
        "        return self.alpha * (x - mean) / (std + self.eps) + self.bias\n"
      ],
      "metadata": {
        "id": "IrqaUiwrPVjg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class FeedForwardBlock(nn.Module):\n",
        "    def __init__(self, d_model: int, d_ff: int, dropout: float) -> None:\n",
        "        super().__init__()\n",
        "        self.linear1 = nn.Linear(d_model, d_ff)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.linear2 = nn.Linear(d_ff, d_model)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.linear2(self.dropout(torch.relu(self.linear1(x))))"
      ],
      "metadata": {
        "id": "8GorhU2mPVl5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiHeadAttentionBlock(nn.Module):\n",
        "    ## h = number of heads\n",
        "    def __init__(self, d_model: int, h: int, dropout: float) -> None:\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "        self.h = h\n",
        "\n",
        "        assert d_model % h == 0, 'd_model is not divisible by h'\n",
        "\n",
        "        self.d_k = d_model // h\n",
        "\n",
        "        self.w_q = nn.Linear(d_model, d_model)\n",
        "        self.w_k = nn.Linear(d_model, d_model)\n",
        "        self.w_v = nn.Linear(d_model, d_model)\n",
        "        self.w_o = nn.Linear(d_model, d_model)\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    @staticmethod\n",
        "    def attention(query, key, value, mask, dropout: nn.Dropout):\n",
        "        d_k = query.shape[-1]\n",
        "\n",
        "\n",
        "        attention_scores = (query @ key.transpose(-2, -1)) / math.sqrt(d_k)\n",
        "\n",
        "\n",
        "        if (mask is not None):\n",
        "            attention_scores.masked_fill_(mask==0, -1e9)\n",
        "        attention_scores = attention_scores.softmax(dim = -1)\n",
        "        if(dropout is not None):\n",
        "            attention_scores = dropout(attention_scores)\n",
        "\n",
        "        return (attention_scores @ value), attention_scores\n",
        "\n",
        "    def forward(self, q, k, v, mask):\n",
        "        query = self.w_q(q)\n",
        "        key = self.w_k(k)\n",
        "        value = self.w_v(v)\n",
        "\n",
        "        query = query.view(query.shape[0], query.shape[1], self.h, self.d_k).transpose(1, 2)\n",
        "        key = key.view(key.shape[0], key.shape[1], self.h, self.d_k).transpose(1, 2)\n",
        "        value = value.view(value.shape[0], value.shape[1], self.h, self.d_k).transpose(1, 2)\n",
        "\n",
        "        x, self.attention_score = MultiHeadAttentionBlock.attention(query, key, value, mask, self.dropout)\n",
        "\n",
        "        x = x.transpose(1, 2).contiguous().view(x.shape[0], -1, self.h * self.d_k)\n",
        "\n",
        "        return self.w_o(x)\n"
      ],
      "metadata": {
        "id": "oBs8p856PVoO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ResidualConnection(nn.Module):\n",
        "    def __init__(self, dropout: float) -> None:\n",
        "        super().__init__()\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.norm = LayerNormalization()\n",
        "\n",
        "    def forward(self, x, sublayer):\n",
        "        return x + self.dropout(sublayer(self.norm(x)))"
      ],
      "metadata": {
        "id": "94ytit1qPVqm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class EncoderBlock(nn.Module):\n",
        "    def __init__(self, self_attention_block: MultiHeadAttentionBlock, feed_forward_block: FeedForwardBlock, dropout: float) -> None:\n",
        "        super().__init__()\n",
        "        self.self_attention_block = self_attention_block\n",
        "        self.feed_forward_block = feed_forward_block\n",
        "        self.residual_connections = nn.ModuleList([ResidualConnection(dropout) for _ in range(2)])\n",
        "\n",
        "    def forward(self, x , src_mask):\n",
        "        x = self.residual_connections[0](x, lambda x: self.self_attention_block(x, x, x, src_mask))\n",
        "\n",
        "        x = self.residual_connections[1](x, self.feed_forward_block)\n",
        "        return x"
      ],
      "metadata": {
        "id": "Mn22K59bbgsb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Encoder(nn.Module):\n",
        "    def __init__(self, layers: nn.ModuleList) -> None:\n",
        "        super().__init__()\n",
        "        self.layers = layers\n",
        "        self.norm = LayerNormalization()\n",
        "\n",
        "    def forward(self, x, mask):\n",
        "        for layer in self.layers:\n",
        "            x = layer(x, mask)\n",
        "        return self.norm(x)"
      ],
      "metadata": {
        "id": "xOcgn02kbgvD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DecoderBlock(nn.Module):\n",
        "    def __init__(self, self_attention_block: MultiHeadAttentionBlock, cross_attention_block: MultiHeadAttentionBlock, feed_forward_block: FeedForwardBlock, dropout: float) -> None:\n",
        "        super().__init__()\n",
        "        self.self_attention_block = self_attention_block\n",
        "        self.cross_attention_block = cross_attention_block\n",
        "        self.feed_forward_block = feed_forward_block\n",
        "        self.residual_connections = nn.ModuleList([ResidualConnection(dropout) for _ in range(3)])\n",
        "\n",
        "    def forward(self, x, encoder_output, src_mask, target_mask):\n",
        "        x = self.residual_connections[0](x, lambda x: self.self_attention_block(x, x, x, target_mask))\n",
        "\n",
        "        x = self.residual_connections[1](x, lambda x: self.cross_attention_block(x, encoder_output, encoder_output, src_mask))\n",
        "\n",
        "        x = self.residual_connections[2](x, self.feed_forward_block)\n",
        "\n",
        "        return x\n"
      ],
      "metadata": {
        "id": "qBhnWlr9bgxj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Decoder(nn.Module):\n",
        "    def __init__(self, layers: nn.ModuleList) -> None:\n",
        "        super().__init__()\n",
        "        self.layers = layers\n",
        "        self.norm = LayerNormalization()\n",
        "\n",
        "    def forward(self, x, encoder_output, src_mask, target_mask):\n",
        "        for layer in self.layers:\n",
        "            x = layer(x, encoder_output, src_mask, target_mask)\n",
        "\n",
        "        return self.norm(x)"
      ],
      "metadata": {
        "id": "JII5VtWneoBA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ProjectionLayer(nn.Module):\n",
        "    def __init__(self, d_model: int, vocab_size: int) -> None:\n",
        "        super().__init__()\n",
        "        self.proj = nn.Linear(d_model, vocab_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return torch.log_softmax(self.proj(x), dim = -1)"
      ],
      "metadata": {
        "id": "oBic6ecfeoDY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Transformer(nn.Module):\n",
        "    def __init__(self, encoder: Encoder, decoder: Decoder, src_embed: InputEmbeddings, target_embed: InputEmbeddings, src_pos: PositionalEncoding, target_pos: PositionalEncoding, projection_layer: ProjectionLayer) -> None:\n",
        "        super().__init__()\n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "        self.src_embed = src_embed\n",
        "        self.target_embed = target_embed\n",
        "        self.src_pos = src_pos\n",
        "        self.target_pos = target_pos\n",
        "        self.projection_layer = projection_layer\n",
        "\n",
        "\n",
        "    ## Encoder\n",
        "    def encode(self, src, src_mask):\n",
        "        src = self.src_embed(src)\n",
        "        src = self.src_pos(src)\n",
        "        return self.encoder(src, src_mask)\n",
        "\n",
        "    ## Decoder\n",
        "    def decode(self, encoder_output, src_mask, target, target_mask):\n",
        "        target = self.target_embed(target)\n",
        "        target = self.target_pos(target)\n",
        "        return self.decoder(target, encoder_output, src_mask, target_mask)\n",
        "\n",
        "    def project(self, x):\n",
        "        return self.projection_layer(x)\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Z9P5vNTteoGB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def build_transformer(src_vocab_size: int, target_vocab_size: int, src_seq_len: int, target_seq_len: int, d_model: int = 512, N: int = 6, H: int = 8, dropout: float = 0.1, d_ff: int = 2048) -> Transformer:\n",
        "    src_embed = InputEmbeddings(d_model, src_vocab_size)\n",
        "    target_embed = InputEmbeddings(d_model, target_vocab_size)\n",
        "\n",
        "    src_pos = PositionalEncoding(d_model, src_seq_len, dropout)\n",
        "    target_pos = PositionalEncoding(d_model, target_seq_len, dropout)\n",
        "\n",
        "    ## create EncoderBlocks\n",
        "    encoder_blocks = []\n",
        "    for _ in range(N):\n",
        "        encoder_self_attention_block = MultiHeadAttentionBlock(d_model, H, dropout)\n",
        "        feed_forward_block = FeedForwardBlock(d_model, d_ff, dropout)\n",
        "\n",
        "        ## Combine layers into an EncoderBlock\n",
        "        encoder_block = EncoderBlock(encoder_self_attention_block, feed_forward_block, dropout)\n",
        "        encoder_blocks.append(encoder_block)\n",
        "\n",
        "    ## create DecodeBlocks\n",
        "    decoder_blocks = []\n",
        "    for _ in range(N):\n",
        "        decoder_self_attention_block = MultiHeadAttentionBlock(d_model, H, dropout)\n",
        "        decoder_cross_attention_block = MultiHeadAttentionBlock(d_model, H, dropout)\n",
        "        feed_forward_block = FeedForwardBlock(d_model, d_ff, dropout)\n",
        "\n",
        "        ## Combine layers into an DecoderBlock\n",
        "        decoder_block = DecoderBlock(decoder_self_attention_block, decoder_cross_attention_block, feed_forward_block, dropout)\n",
        "\n",
        "    encoder = Encoder(nn.ModuleList(encoder_blocks))\n",
        "    decoder = Decoder(nn.ModuleList(decoder_blocks))\n",
        "\n",
        "    ## create projection layer\n",
        "    projection_layer = ProjectionLayer(d_model, target_vocab_size)\n",
        "\n",
        "    ## create the transformer by combining everything above\n",
        "    transformer = Transformer(encoder, decoder, src_embed, target_embed, src_pos, target_pos, projection_layer)\n",
        "\n",
        "    ## Initialize the parameters\n",
        "    for p in transformer.parameters():\n",
        "        if p.dim() > 1:\n",
        "            nn.init.xavier_uniform_(p)\n",
        "\n",
        "    return transformer"
      ],
      "metadata": {
        "id": "SOFyMU_neoKQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def build_tokenizer(config, ds, lang):\n",
        "    tokenizer_path = Path(config['tokenizer_file'].format(lang))\n",
        "\n",
        "    # Checking if Tokenizer already exists\n",
        "    if not Path.exists(tokenizer_path):\n",
        "\n",
        "        # If it doesn't exist, we create a new one\n",
        "        tokenizer = Tokenizer(WordLevel(unk_token = '[UNK]'))\n",
        "        tokenizer.pre_tokenizer = Whitespace()\n",
        "\n",
        "        trainer = WordLevelTrainer(special_tokens = [\"[UNK]\", \"[PAD]\",\n",
        "                                                     \"[SOS]\", \"[EOS]\"], min_frequency = 2)\n",
        "\n",
        "        tokenizer.train_from_iterator(get_all_sentences(ds, lang), trainer = trainer)\n",
        "        tokenizer.save(str(tokenizer_path))\n",
        "    else:\n",
        "        tokenizer = Tokenizer.from_file(str(tokenizer_path))\n",
        "    return tokenizer"
      ],
      "metadata": {
        "id": "hthWT28J-0Yb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Iterating through dataset to extract the original sentence and its translation\n",
        "def get_all_sentences(ds, lang):\n",
        "    for pair in ds:\n",
        "        yield pair['translation'][lang]"
      ],
      "metadata": {
        "id": "UyDEO7n3-0zK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_ds(config):\n",
        "\n",
        "    # Loading the train portion of the OpusBooks dataset.\n",
        "    # The Language pairs will be defined in the 'config' dictionary we will build later\n",
        "    ds_raw = load_dataset('opus_books', f'{config[\"lang_src\"]}-{config[\"lang_target\"]}', split = 'train')\n",
        "\n",
        "    # Building or loading tokenizer for both the source and target languages\n",
        "    tokenizer_src = build_tokenizer(config, ds_raw, config['lang_src'])\n",
        "    tokenizer_target = build_tokenizer(config, ds_raw, config['lang_target'])\n",
        "\n",
        "    # Splitting the dataset for training and validation\n",
        "    train_ds_size = int(0.9 * len(ds_raw)) # 90% for training\n",
        "    val_ds_size = len(ds_raw) - train_ds_size # 10% for validation\n",
        "    train_ds_raw, val_ds_raw = random_split(ds_raw, [train_ds_size, val_ds_size]) # Randomly splitting the dataset\n",
        "\n",
        "    # Processing data with the BilingualDataset class, which we will define below\n",
        "    train_ds = BilingualDataset(train_ds_raw, tokenizer_src, tokenizer_target, config['lang_src'], config['lang_target'], config['seq_len'])\n",
        "    val_ds = BilingualDataset(val_ds_raw, tokenizer_src, tokenizer_target, config['lang_src'], config['lang_target'], config['seq_len'])\n",
        "\n",
        "    # Iterating over the entire dataset and printing the maximum length found in the sentences of both the source and target languages\n",
        "    max_len_src = 0\n",
        "    max_len_target = 0\n",
        "    for pair in ds_raw:\n",
        "        src_ids = tokenizer_src.encode(pair['translation'][config['lang_src']]).ids\n",
        "        target_ids = tokenizer_src.encode(pair['translation'][config['lang_target']]).ids\n",
        "        max_len_src = max(max_len_src, len(src_ids))\n",
        "        max_len_target = max(max_len_target, len(target_ids))\n",
        "\n",
        "    print(f'Max length of source sentence: {max_len_src}')\n",
        "    print(f'Max length of target sentence: {max_len_target}')\n",
        "\n",
        "    # Creating dataloaders for the training and validadion sets\n",
        "    # Dataloaders are used to iterate over the dataset in batches during training and validation\n",
        "    train_dataloader = DataLoader(train_ds, batch_size = config['batch_size'], shuffle = True) # Batch size will be defined in the config dictionary\n",
        "    val_dataloader = DataLoader(val_ds, batch_size = 1, shuffle = True)\n",
        "\n",
        "    return train_dataloader, val_dataloader, tokenizer_src, tokenizer_target # Returning the DataLoader objects and tokenizers\n"
      ],
      "metadata": {
        "id": "TUGZV_Pw-01S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def casual_mask(size):\n",
        "        # Creating a square matrix of dimensions 'size x size' filled with ones\n",
        "        mask = torch.triu(torch.ones(1, size, size), diagonal = 1).type(torch.int)\n",
        "        return mask == 0"
      ],
      "metadata": {
        "id": "_-gZ7OJRbgz0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class BilingualDataset(Dataset):\n",
        "\n",
        "    # This takes in the dataset contaning sentence pairs, the tokenizers for target and source languages, and the strings of source and target languages\n",
        "    # 'seq_len' defines the sequence length for both languages\n",
        "    def __init__(self, ds, tokenizer_src, tokenizer_target, src_lang, target_lang, seq_len) -> None:\n",
        "        super().__init__()\n",
        "\n",
        "        self.seq_len = seq_len\n",
        "        self.ds = ds\n",
        "        self.tokenizer_src = tokenizer_src\n",
        "        self.tokenizer_target = tokenizer_target\n",
        "        self.src_lang = src_lang\n",
        "        self.target_lang = target_lang\n",
        "\n",
        "        # Defining special tokens by using the target language tokenizer\n",
        "        self.sos_token = torch.tensor([tokenizer_target.token_to_id(\"[SOS]\")], dtype=torch.int64)\n",
        "        self.eos_token = torch.tensor([tokenizer_target.token_to_id(\"[EOS]\")], dtype=torch.int64)\n",
        "        self.pad_token = torch.tensor([tokenizer_target.token_to_id(\"[PAD]\")], dtype=torch.int64)\n",
        "\n",
        "\n",
        "    # Total number of instances in the dataset (some pairs are larger than others)\n",
        "    def __len__(self):\n",
        "        return len(self.ds)\n",
        "\n",
        "    # Using the index to retrive source and target texts\n",
        "    def __getitem__(self, index: Any) -> Any:\n",
        "        src_target_pair = self.ds[index]\n",
        "        src_text = src_target_pair['translation'][self.src_lang]\n",
        "        target_text = src_target_pair['translation'][self.target_lang]\n",
        "\n",
        "        # Tokenizing source and target texts\n",
        "        enc_input_tokens = self.tokenizer_src.encode(src_text).ids\n",
        "        dec_input_tokens = self.tokenizer_target.encode(target_text).ids\n",
        "\n",
        "        enc_num_padding_tokens = self.seq_len - len(enc_input_tokens) - 2\n",
        "        # Target tokens\n",
        "        dec_num_padding_tokens = self.seq_len - len(dec_input_tokens) - 1\n",
        "\n",
        "\n",
        "        if enc_num_padding_tokens < 0 or dec_num_padding_tokens < 0:\n",
        "            raise ValueError('Sentence is too long')\n",
        "\n",
        "        encoder_input = torch.cat(\n",
        "            [\n",
        "            self.sos_token, # inserting the '[SOS]' token\n",
        "            torch.tensor(enc_input_tokens, dtype = torch.int64), # Inserting the tokenized source text\n",
        "            self.eos_token, # Inserting the '[EOS]' token\n",
        "            torch.tensor([self.pad_token] * enc_num_padding_tokens, dtype = torch.int64) # Addind padding tokens\n",
        "            ]\n",
        "        )\n",
        "\n",
        "        decoder_input = torch.cat(\n",
        "            [\n",
        "                self.sos_token, # inserting the '[SOS]' token\n",
        "                torch.tensor(dec_input_tokens, dtype = torch.int64), # Inserting the tokenized target text\n",
        "                torch.tensor([self.pad_token] * dec_num_padding_tokens, dtype = torch.int64) # Addind padding tokens\n",
        "            ]\n",
        "\n",
        "        )\n",
        "\n",
        "        # Creating a label tensor, the expected output for training the model\n",
        "        label = torch.cat(\n",
        "            [\n",
        "                torch.tensor(dec_input_tokens, dtype = torch.int64), # Inserting the tokenized target text\n",
        "                self.eos_token, # Inserting the '[EOS]' token\n",
        "                torch.tensor([self.pad_token] * dec_num_padding_tokens, dtype = torch.int64) # Adding padding tokens\n",
        "\n",
        "            ]\n",
        "        )\n",
        "\n",
        "        # Ensuring that the length of each tensor above is equal to the defined 'seq_len'\n",
        "        assert encoder_input.size(0) == self.seq_len\n",
        "        assert decoder_input.size(0) == self.seq_len\n",
        "        assert label.size(0) == self.seq_len\n",
        "\n",
        "        return {\n",
        "            'encoder_input': encoder_input,\n",
        "            'decoder_input': decoder_input,\n",
        "            'encoder_mask': (encoder_input != self.pad_token).unsqueeze(0).unsqueeze(0).int(),\n",
        "            'decoder_mask': (decoder_input != self.pad_token).unsqueeze(0).unsqueeze(0).int() & casual_mask(decoder_input.size(0)),\n",
        "            'label': label,\n",
        "            'src_text': src_text,\n",
        "            'target_text': target_text\n",
        "        }"
      ],
      "metadata": {
        "id": "aFs072Mym2Yl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def greedy_decode(model, source, source_mask, tokenizer_src, tokenizer_target, max_len, device):\n",
        "    sos_idx = tokenizer_target.token_to_id('[SOS]')\n",
        "    eos_idx = tokenizer_target.token_to_id('[EOS]')\n",
        "\n",
        "    encoder_output = model.encode(source, source_mask)\n",
        "    decoder_input = torch.empty(1,1).fill_(sos_idx).type_as(source).to(device)\n",
        "\n",
        "    while True:\n",
        "        if decoder_input.size(1) == max_len:\n",
        "            break\n",
        "\n",
        "        decoder_mask = casual_mask(decoder_input.size(1)).type_as(source_mask).to(device)\n",
        "\n",
        "        out = model.decode(encoder_output, source_mask, decoder_input, decoder_mask)\n",
        "\n",
        "        prob = model.project(out[:, -1])\n",
        "\n",
        "        _, next_word = torch.max(prob, dim=1)\n",
        "        decoder_input = torch.cat([decoder_input, torch.empty(1,1). type_as(source).fill_(next_word.item()).to(device)], dim=1)\n",
        "\n",
        "        if next_word == eos_idx:\n",
        "            break\n",
        "\n",
        "    return decoder_input.squeeze(0)\n"
      ],
      "metadata": {
        "id": "Z3cjBet8_Ny6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def run_validation(model, validation_ds, tokenizer_src, tokenizer_target, max_len, device, print_msg, global_state, writer, num_examples=10):\n",
        "    model.eval()\n",
        "    count = 0\n",
        "\n",
        "    console_width = 80 # Fixed witdh for printed messages\n",
        "\n",
        "    # Creating evaluation loop\n",
        "    with torch.no_grad(): # Ensuring that no gradients are computed during this process\n",
        "        for batch in validation_ds:\n",
        "            count += 1\n",
        "            encoder_input = batch['encoder_input'].to(device)\n",
        "            encoder_mask = batch['encoder_mask'].to(device)\n",
        "\n",
        "            # Ensuring that the batch_size of the validation set is 1\n",
        "            assert encoder_input.size(0) ==  1, 'Batch size must be 1 for validation.'\n",
        "\n",
        "            # Applying the 'greedy_decode' function to get the model's output for the source text of the input batch\n",
        "            model_out = greedy_decode(model, encoder_input, encoder_mask, tokenizer_src, tokenizer_target, max_len, device)\n",
        "\n",
        "            # Retrieving source and target texts from the batch\n",
        "            source_text = batch['src_text'][0]\n",
        "            target_text = batch['target_text'][0] # True translation\n",
        "            model_out_text = tokenizer_target.decode(model_out.detach().cpu().numpy()) # Decoded, human-readable model output\n",
        "\n",
        "            # Printing results\n",
        "            print_msg('-'*console_width)\n",
        "            print_msg(f'SOURCE: {source_text}')\n",
        "            print_msg(f'TARGET: {target_text}')\n",
        "            print_msg(f'PREDICTED: {model_out_text}')\n",
        "\n",
        "            if count == num_examples:\n",
        "                break"
      ],
      "metadata": {
        "id": "mVQCVhjq_N1L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_model(config, vocab_src_len, vocab_target_len):\n",
        "    model = build_transformer(vocab_src_len, vocab_target_len, config['seq_len'], config['seq_len'], config['d_model'])\n",
        "    return model"
      ],
      "metadata": {
        "id": "3eovWiN__Sk6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define settings for building and training the transformer model\n",
        "def get_config():\n",
        "    return{\n",
        "        'batch_size': 8,\n",
        "        'num_epochs': 10,\n",
        "        'lr': 10**-4,\n",
        "        'seq_len': 350,\n",
        "        'd_model': 512, # Dimensions of the embeddings in the Transformer. 512 like in the \"Attention Is All You Need\" paper.\n",
        "        'lang_src': 'en',\n",
        "        'lang_target': 'it',\n",
        "        'model_folder': 'weights',\n",
        "        'model_basename': 'tmodel_',\n",
        "        'preload': None,\n",
        "        'tokenizer_file': 'tokenizer_{0}.json',\n",
        "        'experiment_name': 'runs/tmodel'\n",
        "    }\n",
        "\n",
        "def get_weights_file_path(config, epoch: str):\n",
        "    model_folder = config['model_folder']\n",
        "    model_basename = config['model_basename']\n",
        "    model_filename = f\"{model_basename}{epoch}.pt\"\n",
        "    return str(Path('.')/ model_folder/ model_filename)\n"
      ],
      "metadata": {
        "id": "uzGCMhKqnAoi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_model(config):\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    print(f\"Using device {device}\")\n",
        "\n",
        "    Path(config['model_folder']).mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    train_dataloader, val_dataloader, tokenizer_src, tokenizer_target = get_ds(config)\n",
        "\n",
        "    model = get_model(config,tokenizer_src.get_vocab_size(), tokenizer_target.get_vocab_size()).to(device)\n",
        "\n",
        "    # Tensorboard\n",
        "    writer = SummaryWriter(config['experiment_name'])\n",
        "\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=config['lr'], eps = 1e-9)\n",
        "\n",
        "    initial_epoch = 0\n",
        "    global_step = 0\n",
        "\n",
        "    if config['preload']:\n",
        "        model_filename = get_weights_file_path(config, config['preload'])\n",
        "        print(f'Preloading model {model_filename}')\n",
        "        state = torch.load(model_filename)\n",
        "\n",
        "\n",
        "        initial_epoch = state['epoch'] + 1\n",
        "        optimizer.load_state_dict(state['optimizer_state_dict'])\n",
        "        global_step = state['global_step']\n",
        "\n",
        "\n",
        "    loss_fn = nn.CrossEntropyLoss(ignore_index = tokenizer_src.token_to_id('[PAD]'), label_smoothing = 0.1).to(device)\n",
        "\n",
        "\n",
        "    for epoch in range(initial_epoch, config['num_epochs']):\n",
        "\n",
        "\n",
        "        batch_iterator = tqdm(train_dataloader, desc = f'Processing epoch {epoch:02d}')\n",
        "\n",
        "        for batch in batch_iterator:\n",
        "            model.train()\n",
        "\n",
        "            encoder_input = batch['encoder_input'].to(device)\n",
        "            decoder_input = batch['decoder_input'].to(device)\n",
        "            encoder_mask = batch['encoder_mask'].to(device)\n",
        "            decoder_mask = batch['decoder_mask'].to(device)\n",
        "\n",
        "            encoder_output = model.encode(encoder_input, encoder_mask)\n",
        "            decoder_output = model.decode(encoder_output, encoder_mask, decoder_input, decoder_mask)\n",
        "            proj_output = model.project(decoder_output)\n",
        "\n",
        "            label = batch['label'].to(device)\n",
        "\n",
        "            loss = loss_fn(proj_output.view(-1, tokenizer_target.get_vocab_size()), label.view(-1))\n",
        "\n",
        "            batch_iterator.set_postfix({f\"loss\": f\"{loss.item():6.3f}\"})\n",
        "\n",
        "            writer.add_scalar('train loss', loss.item(), global_step)\n",
        "            writer.flush()\n",
        "\n",
        "            loss.backward()\n",
        "\n",
        "            optimizer.step()\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            global_step += 1\n",
        "\n",
        "        run_validation(model, val_dataloader, tokenizer_src, tokenizer_target, config['seq_len'], device, lambda msg: batch_iterator.write(msg), global_step, writer)\n",
        "\n",
        "        # Saving model\n",
        "        model_filename = get_weights_file_path(config, f'{epoch:02d}')\n",
        "\n",
        "        torch.save({\n",
        "            'epoch': epoch,\n",
        "            'model_state_dict': model.state_dict(),\n",
        "            'optimizer_state_dict': optimizer.state_dict(),\n",
        "            'global_step': global_step\n",
        "        }, model_filename)"
      ],
      "metadata": {
        "id": "1nNZ0IkT90ur"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "if __name__ == '__main__':\n",
        "    warnings.filterwarnings('ignore')\n",
        "    config = get_config()\n",
        "    train_model(config)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rmAb2gkX90xT",
        "outputId": "cb612754-c7d8-4ea5-d010-9dc1bc1f39eb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device cuda\n",
            "Max length of source sentence: 309\n",
            "Max length of target sentence: 274\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing epoch 00: 100%|██████████| 3638/3638 [07:43<00:00,  7.85it/s, loss=5.961]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--------------------------------------------------------------------------------\n",
            "SOURCE: We had made the tea, and were just settling down comfortably to drink it, when George, with his cup half-way to his lips, paused and exclaimed:\n",
            "TARGET: Avevamo fatto il tè, e ci stavamo appunto preparando a sorbirlo comodamente, quando Giorgio, con la tazza innanzi alle labbra, si fermò esclamando:\n",
            "PREDICTED: — Non è vero ?\n",
            "--------------------------------------------------------------------------------\n",
            "SOURCE: \"This parlour is not his sphere,\" I reflected: \"the Himalayan ridge or Caffre bush, even the plague-cursed Guinea Coast swamp would suit him better.\n",
            "TARGET: — Questo salotto non è il suo posto, — pensavo. — Le montagne dell'Himalaya, i paesi del centro dell'Africa, le coste pestilenziali della Guinea gli converrebbero meglio.\n",
            "PREDICTED: — Non è vero ?\n",
            "--------------------------------------------------------------------------------\n",
            "SOURCE: And as there was no one found who united those qualities, it was at any rate better for the post to be filled by an honest rather than a dishonest man.\n",
            "TARGET: E poiché l’uomo che riunisse queste qualità non c’era, era meglio allora che il posto lo occupasse un uomo onesto piuttosto che un disonesto.\n",
            "PREDICTED: — Non è vero ?\n",
            "--------------------------------------------------------------------------------\n",
            "SOURCE: She looked forward joyfully to Dolly's coming with the children, especially because she meant to give each of them their favourite puddings, and because Dolly would appreciate her new arrangements.\n",
            "TARGET: Adesso sognava con gioia l’arrivo di Dolly con i bambini, proprio perché avrebbe potuto ordinare per i bambini i dolci preferiti da ognuno, e perché Dolly avrebbe apprezzato tutta la sua nuova organizzazione.\n",
            "PREDICTED: — Non è vero ?\n",
            "--------------------------------------------------------------------------------\n",
            "SOURCE: 'It matters a good deal to me,' said Alice hastily; 'but I'm not looking for eggs, as it happens; and if I was, I shouldn't want yours: I don't like them raw.'\n",
            "TARGET: — Ma importa moltissimo a me, — rispose subito Alice. — A ogni modo non vado in cerca di uova; e anche se ne cercassi, non ne vorrei delle tue; crude non mi piacciono.\n",
            "PREDICTED: — Non è vero ?\n",
            "--------------------------------------------------------------------------------\n",
            "SOURCE: Shall you be there, Mrs. Fairfax?\"\n",
            "TARGET: Vi sarete, signora Fairfax?\n",
            "PREDICTED: — Non è vero ?\n",
            "--------------------------------------------------------------------------------\n",
            "SOURCE: 'Let those who like it merge to their hearts' content, but it sickens me.'\n",
            "TARGET: — Buon pro’ gli faccia a chi la vuole la fusione; a me non va.\n",
            "PREDICTED: — Non è vero ?\n",
            "--------------------------------------------------------------------------------\n",
            "SOURCE: Eliza's greeting was delivered in a short, abrupt voice, without a smile; and then she sat down again, fixed her eyes on the fire, and seemed to forget me.\n",
            "TARGET: Il saluto di Elisa fu breve e asciutto, non mi sorrise neppure e si sedè di nuovo, fissando il fuoco, come se mi avesse dimenticata.\n",
            "PREDICTED: — Non è vero ?\n",
            "--------------------------------------------------------------------------------\n",
            "SOURCE: With unusual rapidity, as happens at times of great agitation, thoughts and recollections crowded into Dolly's mind.\n",
            "TARGET: Con una rapidità straordinaria, come accade in un momento di agitazione, i pensieri e i ricordi si affollarono nella mente di Dar’ja Aleksandrovna.\n",
            "PREDICTED: — Non è vero ?\n",
            "--------------------------------------------------------------------------------\n",
            "SOURCE: She had, likewise, a fierce and a hard eye: it reminded me of Mrs. Reed's; she mouthed her words in speaking; her voice was deep, its inflections very pompous, very dogmatical,--very intolerable, in short.\n",
            "TARGET: I suoi tratti mi parvero oscurati e solcati dall'orgoglio, che le faceva tener così eretta la testa. Masticava ogni parola.\n",
            "PREDICTED: — Non è vero ?\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing epoch 01:  93%|█████████▎| 3371/3638 [07:10<00:34,  7.85it/s, loss=5.730]"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "U0wRg3Y890z1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "sjs_4pbe902M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "qNkRAjI7904k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Hssjc0OP92OT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Mgs0xC5y92Rb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "A_pIBlyV92S8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "0CpLPwCODlX9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "fzkTGmyXDlaX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "qIilKwp6Dlcv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "8WZogLwoDlfO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "VtHEq0jXDloG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "drqv2O7W92VE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def greedy_decode(model, source, source_mask, tokenizer_src, tokenizer_target, max_len, device):\n",
        "    sos_idx = tokenizer_target.token_to_id(\"[SOS]\")\n",
        "    eos_idx = tokenizer_target.token_to_id(\"[EOS]\")\n",
        "\n",
        "    ## Precompute the encoder output and reuse it for every token we get from the decoder\n",
        "    encoder_output = model.encode(source, source_mask)\n",
        "\n",
        "    ## Initialize the decoder input with the sos token\n",
        "    decoder_input = torch.empty(1, 1).fill_(sos_idx).type_as(source).to(device)\n",
        "    while True:\n",
        "        if(decoder_input.size(1) == max_len):\n",
        "            break\n",
        "\n",
        "        ## build mask for the target (decoder_input)\n",
        "        decoder_mask = casual_mask(decoder_input.size(1)).type_as(source_mask).to(device)\n",
        "\n",
        "        ## Calculate the output of the decoder\n",
        "        out = model.decode(encoder_output, source_mask, decoder_input, decoder_mask)\n",
        "\n",
        "        ## Get the next token\n",
        "        prob = model.project(out[:, -1])\n",
        "\n",
        "        ## Select the token with the max probability (because it is a greedy search)\n",
        "        _, next_word = torch.max(prob, dim=1)\n",
        "        decoder_input = torch.cat([decoder_input, torch.empty(1, 1).type_as(source).fill_(next_word.item()).to(device)], dim=1)\n",
        "\n",
        "        if(next_word == eos_idx):\n",
        "            break\n",
        "\n",
        "    return decoder_input.squeeze(0)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def run_validation(model, validation_ds, tokenizer_src, tokenizer_target, max_len, device, print_msg, global_state, writer, num_examples=2):\n",
        "    model.eval()\n",
        "    count = 0\n",
        "    # source_texts = []\n",
        "    # expected = []\n",
        "    # predicted = []\n",
        "\n",
        "    ## Size of the control window (just use a default value)\n",
        "    console_width = 80\n",
        "    with torch.no_grad():\n",
        "        for batch in validation_ds:\n",
        "            count += 1\n",
        "            encoder_input = batch[\"encoder_input\"].to(device)\n",
        "            encoder_mask = batch[\"encoder_mask\"].to(device)\n",
        "\n",
        "            assert encoder_input.size(0) == 1, \"Batch size must be 1 for validation\"\n",
        "\n",
        "            model_out = greedy_decode(model, encoder_input, encoder_mask, tokenizer_src, tokenizer_target, max_len, device)\n",
        "\n",
        "            source_text = batch[\"src_text\"][0]\n",
        "            target_text = batch[\"target_text\"][0]\n",
        "            model_out_text = tokenizer_target.decode(model_out.detach().cpu().numpy())\n",
        "\n",
        "            # source_text.append(source_text)\n",
        "            # expected.append(target_text)\n",
        "            # predicted.append(model_out_text)\n",
        "\n",
        "            ## Print to the console\n",
        "            print_msg(\"-\" * console_width)\n",
        "            print_msg(\"SOURCE: {}\".format(source_text))\n",
        "            print_msg(\"TARGET: {}\".format(target_text))\n",
        "            print_msg(\"PREDICTED: {}\".format(model_out_text))\n",
        "\n",
        "            if(count == num_examples):\n",
        "                break\n",
        "\n",
        "    # if writer:\n",
        "    #     ## TorchMetrics CharErrorRate, BLEU, WordErrorRate\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def get_all_sentences(ds, lang):\n",
        "    for item in ds:\n",
        "        yield item[\"translation\"][lang]\n",
        "\n",
        "\n",
        "\n",
        "def get_or_build_tokenizer(config, ds, lang):\n",
        "    ## example: config[\"tokenizer_file\"] = \"../tokenizers/tokenizer_{0}.json\"\n",
        "    tokenizer_path = Path(config[\"tokenizer_file\"].format(lang))\n",
        "    if not Path.exists(tokenizer_path):\n",
        "        tokenizer = Tokenizer(WordLevel(unk_token='[UNK]'))\n",
        "        tokenizer.pre_tokenizer = Whitespace()\n",
        "        trainer = WordLevelTrainer(special_tokens=[\"[UNK]\", \"[PAD]\", \"[SOS]\", \"[EOS]\"], min_frequency=2)\n",
        "        tokenizer.train_from_iterator(get_all_sentences(ds, lang), trainer=trainer)\n",
        "        tokenizer.save(str(tokenizer_path))\n",
        "    else:\n",
        "        tokenizer = Tokenizer.from_file(str(tokenizer_path))\n",
        "    return tokenizer\n",
        "\n",
        "\n",
        "# def get_ds(config):\n",
        "#     ds_row = load_dataset(\"opus_books\", f\"{config[\"lang_src\"]}-{config[lang_target]}\", split=\"train\")\n",
        "\n",
        "\n",
        "def get_ds(config):\n",
        "    # ds_raw = load_dataset(\"opus_books\", f'{config[\"lang_src\"]}-{config[\"lang_target\"]}', split=\"train\")\n",
        "    ds_raw = load_dataset(\"cfilt/iitb-english-hindi\", split=\"train\")\n",
        "    print(ds_raw)\n",
        "\n",
        "    ## Build tokenizers\n",
        "    tokenizer_src = get_or_build_tokenizer(config, ds_raw, config[\"lang_src\"])\n",
        "    tokenizer_target = get_or_build_tokenizer(config, ds_raw, config[\"lang_target\"])\n",
        "\n",
        "    ## Split the dataset into 40% training and 10% validation\n",
        "    total_size = len(ds_raw)\n",
        "\n",
        "    # Calculate sizes for training and validation\n",
        "    train_ds_size = int(0.2 * total_size)  # 40% for training\n",
        "    val_ds_size = int(0.1 * total_size)    # 10% for validation\n",
        "    remaining_size = total_size - train_ds_size - val_ds_size  # Remaining 50%\n",
        "\n",
        "    # Split the dataset\n",
        "    train_ds_raw, val_ds_raw, _ = random_split(ds_raw, [train_ds_size, val_ds_size, remaining_size])\n",
        "\n",
        "\n",
        "    train_ds = BilingualDataset(train_ds_raw, tokenizer_src, tokenizer_target, config[\"lang_src\"], config[\"lang_target\"], config[\"seq_len\"])\n",
        "    val_ds = BilingualDataset(val_ds_raw, tokenizer_src, tokenizer_target, config[\"lang_src\"], config[\"lang_target\"], config[\"seq_len\"])\n",
        "\n",
        "    max_len_src = 0\n",
        "    max_len_target = 0\n",
        "\n",
        "    for item in ds_raw:\n",
        "        src_ids = tokenizer_src.encode(item[\"translation\"][config[\"lang_src\"]]).ids\n",
        "        target_ids = tokenizer_target.encode(item[\"translation\"][config[\"lang_target\"]]).ids\n",
        "        max_len_src = max(max_len_src, len(src_ids))\n",
        "        max_len_target = max(max_len_target, len(target_ids))\n",
        "\n",
        "    print(\"Max length of source sentence: {}\".format(max_len_src))\n",
        "    print(\"Max length of target sentence: {}\".format(max_len_target))\n",
        "\n",
        "    train_dataloader = DataLoader(train_ds, batch_size=config[\"batch_size\"], shuffle=True)\n",
        "    val_dataloader = DataLoader(val_ds, batch_size=1, shuffle=True)\n",
        "\n",
        "    return train_dataloader, val_dataloader, tokenizer_src, tokenizer_target\n",
        "\n",
        "\n",
        "\n",
        "def get_model(config, vocab_src_len, vocab_target_len):\n",
        "    model = build_transformer(vocab_src_len, vocab_target_len, config[\"seq_len\"], config[\"seq_len\"], config[\"d_model\"])\n",
        "    return model\n",
        "\n",
        "\n",
        "\n",
        "def train_model(config):\n",
        "    ## Define the device\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    print(f\"Using device {device}\")\n",
        "\n",
        "    Path(config[\"model_folder\"]).mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    train_dataloader, val_dataloader, tokenizer_src, tokenizer_target = get_ds(config)\n",
        "    model = get_model(config, tokenizer_src.get_vocab_size(), tokenizer_target.get_vocab_size()).to(device)\n",
        "\n",
        "    ## Tensorboard\n",
        "    writer = SummaryWriter(config[\"experiment_name\"])\n",
        "\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=config[\"lr\"], eps=1e-9)\n",
        "\n",
        "    initial_epoch = 0\n",
        "    global_step = 0\n",
        "    if(config[\"preload\"]):\n",
        "        model_filename = get_weights_file_path(config, config[\"preload\"])\n",
        "        print(\"Preloading model: {}\".format(model_filename))\n",
        "        state = torch.load(model_filename)\n",
        "        initial_epoch = state[\"epoch\"] + 1\n",
        "        optimizer.load_state_dict(state[\"optimizer_state_dict\"])\n",
        "        global_step = state[\"global_step\"]\n",
        "    loss_fn = nn.CrossEntropyLoss(ignore_index=tokenizer_src.token_to_id(\"[PAD]\"), label_smoothing=0.1)\n",
        "\n",
        "    for epoch in range(initial_epoch, config[\"num_epochs\"]):\n",
        "\n",
        "        batch_iterator = tqdm(train_dataloader, desc=f\"Processing Epoch: {epoch:02d}\")\n",
        "        for batch in batch_iterator:\n",
        "            model.train()\n",
        "\n",
        "            encoder_input = batch[\"encoder_input\"].to(device) ## (Batch, seq_len)\n",
        "            decoder_input = batch[\"decoder_input\"].to(device) ## (Batch, seq_len)\n",
        "            encoder_mask = batch[\"encoder_mask\"].to(device) ## (Batch, 1, 1, seq_len)\n",
        "            decoder_mask = batch[\"decoder_mask\"].to(device) ## (Batch, 1, seq_len, seq_len)\n",
        "\n",
        "            ## Run the tensor through the transformer\n",
        "            encoder_output = model.encode(encoder_input, encoder_mask) ## (Batch, seq_len, d_model)\n",
        "            decoder_output = model.decode(encoder_output, encoder_mask, decoder_input, decoder_mask) ## (Batch, seq_len, d_model)\n",
        "            proj_output = model.project(decoder_output) ## (Batch, seq_len, target_vocab_size)\n",
        "\n",
        "            label = batch[\"label\"].to(device) ## (Batch, seq_len)\n",
        "\n",
        "            ## (Batch, seq_len, target_vocab_size) --> (Batch * seq_len, target_vocab_size)\n",
        "            loss = loss_fn(proj_output.view(-1, tokenizer_target.get_vocab_size()), label.view(-1))\n",
        "            batch_iterator.set_postfix({f\"loss\": f\"{loss.item(): 6.3f}\"})\n",
        "\n",
        "            ## Log the loss on tensorboard\n",
        "            writer.add_scalar(\"train loss\", loss.item(), global_step)\n",
        "            writer.flush()\n",
        "\n",
        "            ## Backpropagate the loss\n",
        "            loss.backward()\n",
        "\n",
        "            ## Update the weights\n",
        "            optimizer.step()\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            #run_validation(model, val_dataloader, tokenizer_src, tokenizer_target, config[\"seq_len\"], device, lambda msg: batch_iterator.write(msg), global_step, writer)\n",
        "\n",
        "            global_step += 1\n",
        "\n",
        "            run_validation(model, val_dataloader, tokenizer_src, tokenizer_target, config[\"seq_len\"], device, lambda msg: batch_iterator.write(msg), global_step, writer)\n",
        "\n",
        "\n",
        "        ## Save the model at the end of every epoch\n",
        "        model_filename = get_weights_file_path(config, f\"{epoch:02d}\")\n",
        "        torch.save({\n",
        "            \"epoch\": epoch,\n",
        "            \"mode_state_dict\": model.state_dict(),\n",
        "            \"optimizer_state_dict\": optimizer.state_dict(),\n",
        "            \"global_step\": global_step\n",
        "        }, model_filename)\n",
        "\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    warnings.filterwarnings(\"ignore\")\n",
        "    config = get_config()\n",
        "    train_model(config)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 443
        },
        "id": "RMebZhElnJnp",
        "outputId": "8e0afea7-e61e-4269-ea96-f1272ae4b44a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device cuda\n",
            "Dataset({\n",
            "    features: ['translation'],\n",
            "    num_rows: 1659083\n",
            "})\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-124-7f03fc134f96>\u001b[0m in \u001b[0;36m<cell line: 230>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    231\u001b[0m     \u001b[0mwarnings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfilterwarnings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"ignore\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    232\u001b[0m     \u001b[0mconfig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_config\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 233\u001b[0;31m     \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    234\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    235\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-124-7f03fc134f96>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(config)\u001b[0m\n\u001b[1;32m    158\u001b[0m     \u001b[0mPath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"model_folder\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmkdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparents\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexist_ok\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    159\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 160\u001b[0;31m     \u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_dataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer_src\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer_target\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_ds\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    161\u001b[0m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer_src\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_vocab_size\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer_target\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_vocab_size\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    162\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-124-7f03fc134f96>\u001b[0m in \u001b[0;36mget_ds\u001b[0;34m(config)\u001b[0m\n\u001b[1;32m    131\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mitem\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mds_raw\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m         \u001b[0msrc_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer_src\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"translation\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"lang_src\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mids\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 133\u001b[0;31m         \u001b[0mtarget_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer_target\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"translation\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"lang_target\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mids\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    134\u001b[0m         \u001b[0mmax_len_src\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_len_src\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    135\u001b[0m         \u001b[0mmax_len_target\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_len_target\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "7-f0B2htnJqD"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}